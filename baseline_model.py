import torch
import torch.nn as nn
import torchvision.models as models
import torch.nn.functional as F

class CNN_GRU(nn.Module):
    """
        A baseline eye tracking which uses CNN + GRU to predict the pupil center coordinate
    """
    def __init__(self, args):
        super().__init__() 
        self.args = args
        self.conv1 = nn.Conv2d(args.n_time_bins, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)
        self.gru = nn.GRU(input_size=36192, hidden_size=128, num_layers=1, batch_first=True)
        self.fc = nn.Linear(128, 2)


    def forward(self, x):
        # input is of shape (batch_size, seq_len, channels, height, width)
        batch_size, seq_len, channels, height, width = x.shape
        x = x.view(batch_size*seq_len, channels, height, width)
        # permute height and width
        x = x.permute(0, 1, 3, 2)

        x= self.conv1(x)
        x= torch.relu(x)
        x= self.conv2(x)
        x= torch.relu(x)
        x= self.conv3(x)
        x= torch.relu(x)
        x= self.pool(x)

        x = x.view(batch_size, seq_len, -1)
        x, _ = self.gru(x)
        # output shape of x is (batch_size, seq_len, hidden_size)

        x = self.fc(x)
        # output is of shape (batch_size, seq_len, 2)
        return x
    


class Resnet_GRU(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args

        # Load ResNet18 pretrained, bá» layer cuá»‘i (fc)
        resnet = models.resnet18(pretrained=True)
        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Loáº¡i bá» FC layer

        # GRU xá»­ lÃ½ chuá»—i thá»i gian
        self.gru = nn.GRU(input_size=512, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(128 * 2, 2)  # Bidirectional nÃªn nhÃ¢n Ä‘Ã´i hidden_size

    def forward(self, x):
        batch_size, seq_len, channels, height, width = x.shape
        x = x.view(batch_size * seq_len, channels, height, width)

        x = self.cnn(x)  # Feature shape: (batch_size*seq_len, 512, 1, 1)
        x = x.view(batch_size, seq_len, -1)  # (batch_size, seq_len, 512)

        x, _ = self.gru(x)  # (batch_size, seq_len, 256)
        x = self.fc(x)  # (batch_size, seq_len, 2)
        return x

class Resnet_LSTM(nn.Module):
    def __init__(self, args):
        super(Resnet_LSTM, self).__init__()
        self.args = args
        
        # Load ResNet18 pretrained vÃ  loáº¡i bá» lá»›p fully-connected cuá»‘i cÃ¹ng
        resnet = models.resnet18(pretrained=True)
        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Káº¿t quáº£: (batch_size*seq_len, 512, 1, 1)
        
        # LSTM Ä‘á»ƒ xá»­ lÃ½ chuá»—i thá»i gian. Sá»­ dá»¥ng LSTM 2 táº§ng, bidirectional vá»›i hidden_size=128
        self.lstm = nn.LSTM(input_size=512, hidden_size=128, num_layers=2, 
                            batch_first=True, bidirectional=True)
        
        # Fully-connected layer Ä‘á»ƒ chuyá»ƒn Ä‘á»•i Ä‘áº§u ra cá»§a LSTM thÃ nh tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(128 * 2, 2)  # bidirectional nÃªn nhÃ¢n Ä‘Ã´i hidden_size

    def forward(self, x):
        # x cÃ³ shape: (batch_size, seq_len, channels, height, width)
        batch_size, seq_len, channels, height, width = x.shape
        
        # Gá»™p batch_size vÃ  seq_len Ä‘á»ƒ Ä‘Æ°a vÃ o CNN
        x = x.view(batch_size * seq_len, channels, height, width)
        x = self.cnn(x)  # Shape: (batch_size*seq_len, 512, 1, 1)
        x = x.view(batch_size, seq_len, -1)  # Shape: (batch_size, seq_len, 512)
        
        # Xá»­ lÃ½ chuá»—i vá»›i LSTM
        x, _ = self.lstm(x)  # Shape: (batch_size, seq_len, 256)
        
        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™ (x, y) cho má»—i frame
        x = self.fc(x)  # Shape: (batch_size, seq_len, 2)
        return x


class ViT_GRU(nn.Module):
    def __init__(self, args):
        super(ViT_GRU, self).__init__()
        self.args = args
        
        # Láº¥y mÃ´ hÃ¬nh Vision Transformer tá»« torchvision (ViT-B/16)
        self.vit = models.vit_b_16(pretrained=True)
        
        # Láº¥y kÃ­ch thÆ°á»›c Ä‘áº§u ra cá»§a ViT
        feature_dim = self.vit.heads[0].in_features  # âœ… CÃ¡ch Ä‘Ãºng Ä‘á»ƒ láº¥y feature_dim
        
        # Bá» classifier Ä‘á»ƒ láº¥y Ä‘áº·c trÆ°ng thÃ´
        self.vit.heads = nn.Identity()

        # GRU Ä‘á»ƒ xá»­ lÃ½ chuá»—i thá»i gian
        self.gru = nn.GRU(input_size=feature_dim, hidden_size=128, num_layers=2, 
                          batch_first=True, bidirectional=True)
        
        # Fully connected Ä‘á»ƒ chuyá»ƒn GRU output thÃ nh tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(128 * 2, 2)  # Bidirectional nÃªn nhÃ¢n Ä‘Ã´i hidden_size
        
    def forward(self, x):
        batch_size, seq_len, channels, height, width = x.shape
    
        # Resize input vá» 224x224
        x = x.view(batch_size * seq_len, channels, height, width)
        x = F.interpolate(x, size=(224, 224), mode="bilinear", align_corners=False)
    
        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« ViT
        features = self.vit(x)
        
        # Reshape láº¡i thÃ nh chuá»—i thá»i gian
        features = features.view(batch_size, seq_len, -1)
        
        # ÄÆ°a vÃ o GRU
        gru_out, _ = self.gru(features)
        
        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™
        out = self.fc(gru_out)
        return out

class SwinT_GRU(nn.Module):
    def __init__(self, args):
        super(SwinT_GRU, self).__init__()
        self.args = args
        
        # Load mÃ´ hÃ¬nh Swin Transformer tá»« torchvision (Swin-Tiny)
        self.swin = models.swin_t(pretrained=True)
        
        # Láº¥y kÃ­ch thÆ°á»›c Ä‘áº§u ra cá»§a Swin Transformer (thÆ°á»ng lÃ  768)
        feature_dim = self.swin.head.in_features  # Swin-T output features
        
        # Loáº¡i bá» classifier cuá»‘i cá»§a Swin Transformer
        self.swin.head = nn.Identity()
        
        # GRU Ä‘á»ƒ xá»­ lÃ½ chuá»—i thá»i gian
        self.gru = nn.GRU(input_size=feature_dim, hidden_size=128, num_layers=2, 
                          batch_first=True, bidirectional=True)
        
        # Fully connected Ä‘á»ƒ chuyá»ƒn GRU output thÃ nh tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(128 * 2, 2)  # Bidirectional nÃªn nhÃ¢n Ä‘Ã´i hidden_size
        
    def forward(self, x):
        """
        Äáº§u vÃ o x cÃ³ shape: (batch_size, seq_len, channels, height, width)
        """
        batch_size, seq_len, channels, height, width = x.shape
        
        # Gá»™p batch_size vÃ  seq_len láº¡i Ä‘á»ƒ Ä‘Æ°a vÃ o Swin Transformer
        x = x.view(batch_size * seq_len, channels, height, width)
        
        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khÃ´ng gian tá»« Swin Transformer
        features = self.swin(x)  # Shape: (batch_size*seq_len, feature_dim)
        
        # Reshape láº¡i thÃ nh chuá»—i theo thá»i gian: (batch_size, seq_len, feature_dim)
        features = features.view(batch_size, seq_len, -1)
        
        # Xá»­ lÃ½ chuá»—i vá»›i GRU
        gru_out, _ = self.gru(features)  # Output: (batch_size, seq_len, 256)
        
        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™ (x, y) cho tá»«ng frame
        out = self.fc(gru_out)  # Shape: (batch_size, seq_len, 2)
        return out

class EfficientNet_GRU(nn.Module):
    def __init__(self, args):
        super(EfficientNet_GRU, self).__init__()
        self.args = args
        
        # Load EfficientNet-B0 pretrained
        self.effnet = models.efficientnet_b0(pretrained=True)
        
        # Láº¥y feature size tá»« EfficientNet
        feature_dim = self.effnet.classifier[1].in_features
        
        # Bá» fully-connected layer cuá»‘i
        self.effnet.classifier = nn.Identity()
        
        # GRU Ä‘á»ƒ xá»­ lÃ½ chuá»—i thá»i gian
        self.gru = nn.GRU(input_size=feature_dim, hidden_size=128, num_layers=2, 
                          batch_first=True, bidirectional=True)
        
        # Fully connected Ä‘á»ƒ chuyá»ƒn GRU output thÃ nh tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(128 * 2, 2)  # Bidirectional nÃªn nhÃ¢n Ä‘Ã´i hidden_size
        
    def forward(self, x):
        """
        Äáº§u vÃ o x cÃ³ shape: (batch_size, seq_len, channels, height, width)
        """
        batch_size, seq_len, channels, height, width = x.shape
        
        # Gá»™p batch_size vÃ  seq_len láº¡i Ä‘á»ƒ Ä‘Æ°a vÃ o EfficientNet
        x = x.view(batch_size * seq_len, channels, height, width)
        
        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khÃ´ng gian tá»« EfficientNet
        features = self.effnet(x)  # Shape: (batch_size*seq_len, feature_dim)
        
        # Reshape láº¡i thÃ nh chuá»—i theo thá»i gian: (batch_size, seq_len, feature_dim)
        features = features.view(batch_size, seq_len, -1)
        
        # Xá»­ lÃ½ chuá»—i vá»›i GRU
        gru_out, _ = self.gru(features)  # Output: (batch_size, seq_len, 256)
        
        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™ (x, y) cho tá»«ng frame
        out = self.fc(gru_out)  # Shape: (batch_size, seq_len, 2)
        return out

class EfficientNetB3_GRU(nn.Module):
    def __init__(self, args):
        super(EfficientNetB3_GRU, self).__init__()
        self.args = args
        
        # Load EfficientNet-B3 pretrained
        self.effnet = models.efficientnet_b3(pretrained=True)
        
        # Láº¥y feature size tá»« EfficientNet
        feature_dim = self.effnet.classifier[1].in_features
        
        # Bá» fully-connected layer cuá»‘i
        self.effnet.classifier = nn.Identity()
        
        # GRU Ä‘á»ƒ xá»­ lÃ½ chuá»—i thá»i gian
        self.gru = nn.GRU(input_size=feature_dim, hidden_size=128, num_layers=2, 
                          batch_first=True, bidirectional=True)
        
        # Fully connected Ä‘á»ƒ chuyá»ƒn GRU output thÃ nh tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(128 * 2, 2)  # Bidirectional nÃªn nhÃ¢n Ä‘Ã´i hidden_size
        
    def forward(self, x):
        """
        Äáº§u vÃ o x cÃ³ shape: (batch_size, seq_len, channels, height, width)
        """
        batch_size, seq_len, channels, height, width = x.shape
        
        # Gá»™p batch_size vÃ  seq_len láº¡i Ä‘á»ƒ Ä‘Æ°a vÃ o EfficientNet
        x = x.view(batch_size * seq_len, channels, height, width)
        
        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khÃ´ng gian tá»« EfficientNet
        features = self.effnet(x)  # Shape: (batch_size*seq_len, feature_dim)
        
        # Reshape láº¡i thÃ nh chuá»—i theo thá»i gian: (batch_size, seq_len, feature_dim)
        features = features.view(batch_size, seq_len, -1)
        
        # Xá»­ lÃ½ chuá»—i vá»›i GRU
        gru_out, _ = self.gru(features)  # Output: (batch_size, seq_len, 256)
        
        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™ (x, y) cho tá»«ng frame
        out = self.fc(gru_out)  # Shape: (batch_size, seq_len, 2)
        return out

class EfficientNet_LSTM(nn.Module):
    def __init__(self, args):
        super(EfficientNet_LSTM, self).__init__()
        self.args = args
        
        # Load EfficientNet-B0 pretrained tá»« torchvision
        self.effnet = models.efficientnet_b0(pretrained=True)
        
        # Láº¥y sá»‘ lÆ°á»£ng features tá»« classifier cá»§a EfficientNet
        feature_dim = self.effnet.classifier[1].in_features
        
        # Bá» classifier cuá»‘i cÃ¹ng Ä‘á»ƒ chá»‰ láº¥y feature
        self.effnet.classifier = nn.Identity()
        
        # LSTM thay vÃ¬ GRU
        self.lstm = nn.LSTM(input_size=feature_dim, hidden_size=128, num_layers=2, 
                            batch_first=True, bidirectional=True)
        
        # Fully connected Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(128 * 2, 2)  # NhÃ¢n Ä‘Ã´i hidden_size vÃ¬ bidirectional
        
    def forward(self, x):
        """
        Äáº§u vÃ o x cÃ³ dáº¡ng: (batch_size, seq_len, channels, height, width)
        """
        batch_size, seq_len, channels, height, width = x.shape
        
        # Gá»™p batch_size vÃ  seq_len Ä‘á»ƒ Ä‘Æ°a vÃ o EfficientNet
        x = x.view(batch_size * seq_len, channels, height, width)
        
        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« EfficientNet
        features = self.effnet(x)  # Shape: (batch_size*seq_len, feature_dim)
        
        # Reshape láº¡i thÃ nh dáº¡ng chuá»—i thá»i gian
        features = features.view(batch_size, seq_len, -1)
        
        # Xá»­ lÃ½ báº±ng LSTM
        lstm_out, _ = self.lstm(features)  # Output: (batch_size, seq_len, 256)
        
        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™ máº¯t (x, y)
        out = self.fc(lstm_out)  # Shape: (batch_size, seq_len, 2)
        
        return out



class Attention(nn.Module):
    def __init__(self, hidden_size):
        super(Attention, self).__init__()
        self.attn = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, x):
        """
        x cÃ³ shape: (batch_size, seq_len, hidden_size)
        """
        attn_weights = torch.softmax(self.attn(x), dim=1)  
        context = attn_weights * x  # Giá»¯ seq_len
        return context  # Shape: (batch_size, seq_len, hidden_size)

class EfficientNet_GRU_Attention(nn.Module):
    def __init__(self, args):
        super(EfficientNet_GRU_Attention, self).__init__()
        self.args = args
        
        # Load EfficientNet-B0 pretrained
        self.effnet = models.efficientnet_b0(pretrained=True)
        
        # Láº¥y feature size tá»« EfficientNet
        feature_dim = self.effnet.classifier[1].in_features
        
        # Bá» fully-connected layer cuá»‘i
        self.effnet.classifier = nn.Identity()
        
        # GRU Ä‘á»ƒ xá»­ lÃ½ chuá»—i thá»i gian
        self.gru = nn.GRU(input_size=feature_dim, hidden_size=128, num_layers=2, 
                          batch_first=True, bidirectional=True)
        
        # ThÃªm Attention Ä‘á»ƒ trá»ng sá»‘ hÃ³a thÃ´ng tin tá»« GRU
        self.attention = Attention(128 * 2)  # GRU bidirectional nÃªn hidden_size x2
        
        # Fully connected Ä‘á»ƒ chuyá»ƒn GRU output thÃ nh tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(128 * 2, 2) 
        
    def forward(self, x):
        """
        Äáº§u vÃ o x cÃ³ shape: (batch_size, seq_len, channels, height, width)
        """
        batch_size, seq_len, channels, height, width = x.shape
        
        # Gá»™p batch_size vÃ  seq_len láº¡i Ä‘á»ƒ Ä‘Æ°a vÃ o EfficientNet
        x = x.view(batch_size * seq_len, channels, height, width)
        
        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khÃ´ng gian tá»« EfficientNet
        features = self.effnet(x)  # Shape: (batch_size*seq_len, feature_dim)
        
        # Reshape láº¡i thÃ nh chuá»—i theo thá»i gian: (batch_size, seq_len, feature_dim)
        features = features.view(batch_size, seq_len, -1)
        
        # Xá»­ lÃ½ chuá»—i vá»›i GRU
        gru_out, _ = self.gru(features)  # Shape: (batch_size, seq_len, 256)
        
        # Ãp dá»¥ng Attention Ä‘á»ƒ láº¥y trá»ng sá»‘ quan trá»ng
        context = self.attention(gru_out)  # Shape: (batch_size, 256)
        
        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™ (x, y)
        out = self.fc(context)  # Shape: (batch_size, 2)


        return out

class EfficientNet_BiGRU_S6(nn.Module):
    def __init__(self, args):
        super(EfficientNet_BiGRU_S6, self).__init__()
        self.args = args
        
        # Load EfficientNet-B0 pretrained
        self.effnet = models.efficientnet_b0(pretrained=True)
        
        # Láº¥y feature size tá»« EfficientNet
        feature_dim = self.effnet.classifier[1].in_features
        
        # Bá» fully-connected layer cuá»‘i
        self.effnet.classifier = nn.Identity()
        
        # GRU xá»­ lÃ½ chuá»—i thá»i gian (Bidirectional)
        self.gru = nn.GRU(input_size=feature_dim, hidden_size=128, num_layers=2, 
                          batch_first=True, bidirectional=True)
        
        # Thay Attention báº±ng Mamba (S6-based) Ä‘á»ƒ xá»­ lÃ½ thÃ´ng tin chuá»—i dÃ i
        self.s6 = Mamba(d_model=256, d_state=64, d_conv=4, expand=2)  # 256 do GRU bidirectional
        
        # Fully connected Ä‘á»ƒ chuyá»ƒn output thÃ nh tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(256, 2)  # 256 do BiGRU Ä‘áº§u ra (128 x2)

        # Layer Normalization giÃºp á»•n Ä‘á»‹nh Ä‘áº§u vÃ o cho S6
        self.norm = nn.LayerNorm(256)
        
    def forward(self, x):
        """
        Äáº§u vÃ o x cÃ³ shape: (batch_size, seq_len, channels, height, width)
        """
        batch_size, seq_len, channels, height, width = x.shape
        
        # Gá»™p batch_size vÃ  seq_len Ä‘á»ƒ Ä‘Æ°a vÃ o EfficientNet
        x = x.view(batch_size * seq_len, channels, height, width)
        
        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khÃ´ng gian tá»« EfficientNet
        features = self.effnet(x)  # Shape: (batch_size*seq_len, feature_dim)
        
        # Reshape láº¡i thÃ nh chuá»—i theo thá»i gian: (batch_size, seq_len, feature_dim)
        features = features.view(batch_size, seq_len, -1)
        
        # Xá»­ lÃ½ chuá»—i vá»›i BiGRU
        gru_out, _ = self.gru(features)  # Shape: (batch_size, seq_len, 256)
        
        # Chuáº©n hÃ³a Ä‘áº§u vÃ o cho S6
        gru_out = self.norm(gru_out)
        
        # Ãp dá»¥ng S6 (Mamba) Ä‘á»ƒ xá»­ lÃ½ thÃ´ng tin chuá»—i dÃ i
        s6_out = self.s6(gru_out)  # Shape: (batch_size, seq_len, 256)
        
        # Residual Connection: Giá»¯ láº¡i thÃ´ng tin tá»« BiGRU
        s6_out = s6_out + gru_out  # Shape: (batch_size, seq_len, 256)
        
        # Láº¥y thÃ´ng tin cuá»‘i cÃ¹ng (táº¡i thá»i Ä‘iá»ƒm seq_len) Ä‘á»ƒ dá»± Ä‘oÃ¡n
        final_out = s6_out[:, -1, :]  # Shape: (batch_size, 256)
        
        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™ (x, y)
        out = self.fc(final_out)  # Shape: (batch_size, 2)
        
        return out

class ConvNeXt_BiGRU(nn.Module):
    def __init__(self, args):
        super(ConvNeXt_BiGRU, self).__init__()
        self.args = args
        
        # ğŸ”¥ DÃ¹ng ConvNeXt lÃ m backbone thay vÃ¬ EfficientNet
        self.backbone = models.convnext_base(weights="IMAGENET1K_V1")
        feature_dim = self.backbone.classifier[2].in_features
        
        # Bá» fully-connected cuá»‘i
        self.backbone.classifier = nn.Identity()
        
        # GRU xá»­ lÃ½ chuá»—i thá»i gian (Bidirectional)
        self.gru = nn.GRU(input_size=feature_dim, hidden_size=128, num_layers=2, 
                          batch_first=True, bidirectional=True)
        
        # Fully connected Ä‘á»ƒ chuyá»ƒn output thÃ nh tá»a Ä‘á»™ (x, y)
        self.fc = nn.Linear(256, 2)  # 256 do BiGRU Ä‘áº§u ra (128 x2)

    def forward(self, x):
        batch_size, seq_len, channels, height, width = x.shape

        # Gá»™p batch_size vÃ  seq_len Ä‘á»ƒ Ä‘Æ°a vÃ o ConvNeXt
        x = x.view(batch_size * seq_len, channels, height, width)

        # ğŸ”¥ TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng báº±ng ConvNeXt
        features = self.backbone(x)  # Shape: (batch_size*seq_len, feature_dim)

        # Reshape láº¡i thÃ nh chuá»—i theo thá»i gian: (batch_size, seq_len, feature_dim)
        features = features.view(batch_size, seq_len, -1)

        # Xá»­ lÃ½ chuá»—i vá»›i BiGRU
        gru_out, _ = self.gru(features)  # Shape: (batch_size, seq_len, 256)

        # Láº¥y thÃ´ng tin cuá»‘i cÃ¹ng (táº¡i thá»i Ä‘iá»ƒm seq_len) Ä‘á»ƒ dá»± Ä‘oÃ¡n
        final_out = gru_out[:, -1, :]  # Shape: (batch_size, 256)

        # Dá»± Ä‘oÃ¡n tá»a Ä‘á»™ (x, y)
        out = self.fc(final_out)  # Shape: (batch_size, 2)

        return out
